# Get Started

<!--
**Sample Description**: Provide a brief overview of the application and its purpose.
-->
The **Search Image by Image** application is a reference application that demonstrates how developers can leverage edge AI technologies to solve real-world challenges. It enables efficient processing and searching of video data to identify objects of interest, providing actionable insights in real-time.

This reference application showcases searching through video data generated by cameras around a city to find the closest match to the user-provided image.

<!--
**What You Can Do**: Highlight the developer workflows supported by the guide.
-->
By following this guide, you will learn how to:
- **Set up the sample application**: Use Docker Compose to quickly deploy the application in your environment.
- **Run a predefined pipeline**: Execute a sample pipeline to see real-time traffic monitoring and object detection in action.
- **Modify application parameters**: Customize settings like input sources and detection thresholds to adapt the application to your specific requirements.


## Prerequisites
- Verify that your system meets the [minimum requirements](./system-requirements.md).
- Install Docker: [Installation Guide](https://docs.docker.com/get-docker/).

<!--
**Setup and First Use**: Include installation instructions, basic operation, and initial validation.
-->
## Set up and First Use

<!--
**User Story 1**: Setting Up the Application  
- **As a developer**, I want to set up the application in my environment, so that I can start exploring its functionality.

**Acceptance Criteria**:
1. Step-by-step instructions for downloading and installing the application.
2. Verification steps to ensure successful setup.
3. Troubleshooting tips for common installation issues.
-->


1. **Download the Compose File**:
    - Download the Docker Compose file:
      ```bash
      curl -O https:.../search_image_by_image/docker-compose.yaml # The address is going to change
      ```

2. **Navigate to the Directory**:
    - Go to the directory where you saved the Compose file:
      ```bash
      cd /path/to/directory
      ```
<!--
a pre-step to prepare models may be needed
-->

3. **Start the Application**:
    - Run the application using Docker Compose:
      ```bash
      docker compose up -d
      ```

4. **Verify the Application**:
    - Check that the application is running:
      ```bash
      docker ps
      ```

5. **Access the Application**:
    - Open a browser and go to the following endpoints to access the application:
      - Stream UI: `http://localhost:8889/stream`
      - App UI: `http://localhost:3000`
      - Search UI: `http://localhost:9000/docs`
      - Milvus UI: `http://localhost:8000/`


6. **Run the Application**:

    - **Analyze Stream**: Use the predefined video and click **Analyze Stream** to start processing the video stream.
    - **Video Search**: Click the **Upload Image** button to upload your own images for search or click the **Capture Frame** button to capture and adjust frames from the video stream. Click the **Search Object** button.

    - **Expected Results**:
      - Matched search results, including metadata, timestamps, distance to show the confidence rate of the prediction, and frames that include detected objects (e.g., vehicles, pedestrians, bikes).
    <div align="center">
        <img src="./images/imagesearch1.png" width="45%" style="margin-right:1rem"/>
        <img src="./images/imagesearch2.png" width="45%" />
    </div>
<!--

**Modify Basic Parameters**: Explain configurable options and their impacts.

## Modify Application Parameters
<!--**User Story 4**: Modifying Basic Configurations  
- **As a developer**, I want to adjust simple configurations (e.g., sensor inputs or thresholds), so that I can explore the applicationâ€™s flexibility.

**Acceptance Criteria**:
1. A list of configurable parameters and their descriptions.
2. Examples of modifying key settings.
3. Steps to verify and test modified configurations.


### Basic Parameters
<!--
1. Begin with a table listing key parameters, their purpose, and acceptable values.
2. Include concise explanations for why a developer might modify each parameter.


| **Parameter**          | **Objective**                                                | **Expected Values/Range**          |
|-------------------------|-------------------------------------------------------------|-------------------------------------|
| `input_source`          | Change the input device (e.g., local camera, network feed). | `/dev/video0`, `rtsp://<camera-ip>` |
| `detection_threshold`   | Adjust detection sensitivity.                               | `0.3` to `0.8`                      |
| `repeat_count`          | Control pipeline iterations for performance testing.        | Integer values (e.g., `1`, `5`)     |

---

### Modify Parameters
1. **Find the Configuration File**:
   - Navigate to the configuration directory:
     ```bash
     cd /path/to/configuration
     ```

2. **Edit Parameters**:
   - Open the configuration file:
     ```json
     {
         "pipeline": {
             "input_source": "/dev/video0",
             "detection_threshold": 0.5,
             "repeat_count": 1
         }
     }
     ```

3. **Modify Key Parameters**:
   - Change the `input_source` to a network camera:
     ```json
     "input_source": "rtsp://<camera-ip>"
     ```
   - Adjust the `detection_threshold` to reduce false positives:
     ```json
     "detection_threshold": 0.7
     ```
--> 
**Make Changes**

1. **Change Object Detection and Object Classification Models**

To use your own models instead of the default models, follow these steps:

  - Open the `config.json` file located at `src/evam/configs/filter-pipeline/config.json`.

  - Change the paths in the `pipeline` section to point to your own models. Replace the paths for `gvadetect` and `gvaclassify` with the paths to your models:
    ```json
    {
        "config": {
            "logging": {
                "C_LOG_LEVEL": "INFO",
                "PY_LOG_LEVEL": "INFO"
            },
            "pipelines": [
                {
                    "name": "filter-pipeline",
                    "source": "gstreamer",
                    "queue_maxsize": 50,
                    "pipeline": "{auto_source} name=source ! decodebin ! video/x-raw ! videoconvert ! gvadetect model=/models/your-detection-model/FP32/your-detection-model.xml model-proc=/models/your-detection-model/your-detection-model.json inference-interval=3 threshold=0.4 device=AUTO ! gvaclassify model=/models/your-classification-model/FP32/your-classification-model.xml model-proc=/models/your-classification-model/your-classification-model.json name=classification ! queue ! videoconvertscale ! gvametaconvert name=metaconvert ! gvapython class=MQTTPublisher function=process module=/home/pipeline-server/gvapython/mqtt_publisher/mqtt_publisher.py name=mqtt_publisher ! gvametapublish ! appsink name=destination",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "mqtt_publisher": {
                                "element": {
                                    "name": "mqtt_publisher",
                                    "property": "kwarg",
                                    "format": "json"
                                },
                                "type": "object"
                            }
                        }
                    },
                    "auto_start": false
                },
                {
                    "name": "search_image",
                    "source": "image_ingestor",
                    "queue_maxsize": 50,
                    "pipeline": "appsrc name=source  ! decodebin ! videoconvert ! gvainference model=/models/your-classification-model/FP32/your-classification-model.xml device=CPU ! gvametaconvert ! appsink name=destination"
                }
            ]
        }
    }
    ```

2. **Change Input Video**:

To use your own video instead of the default sample video, follow these steps:

  - Locate the `streaming-pipeline` Service and open the `compose.yml` file and find the `streaming-pipeline` service.
  - Change the URL in the `command` section to point to your own video file. Replace `https://github.com/intel-iot-devkit/sample-videos/raw/master/person-bicycle-car-detection.mp4` with the URL of your video file:
     ```yaml
     services:
       streaming-pipeline:
         ...
         command: >
           bash -c "
             wget -O file.mp4 <your-video-url> && \
             gst-launch-1.0 filesrc location=file.mp4 ! qtdemux name=mdemux ! h264parse ! video/x-h264,stream-format=byte-stream ! mpegtsmux name=mux ! filesink location=file.ts && \
             gst-launch-1.0 multifilesrc location=file.ts loop=true ! tsdemux ! h264parse ! rtspclientsink protocols=tcp location=rtsp://rtsp-server:8554/stream
           "
         ...
     ```

3. **Adjust Feature Matching Confidence**:
   - Open the compose.yml file and locate the `feature-matching` service.
   - Change the `CONFIDENCE_THRESHOLD` to adjust the confidence level for feature matching:
     ```yaml
     services:
       feature-matching:
         ...
         environment:
           ...
           CONFIDENCE_THRESHOLD: 0.7
         ...
     ```
   - **Effect**: Increasing the `CONFIDENCE_THRESHOLD` will make the feature matching more stringent, reducing false positives but potentially missing some true positives. Decreasing it will make the matching more lenient, increasing the chances of detecting true positives but also increasing false positives.

4. **Save Changes and Restart**:
   - Save the file and restart the application:
     ```bash
     docker-compose restart
     ```

5. **Verify Updates**:
   - **Expected Results**:
     - The application processes data from the updated input source.
     - Detection results align with the changed models
   - Confirm changes through:
     - Logs:
       ```bash
       docker-compose logs
       ```


## Troubleshooting

1. **Containers Not Starting**:
   - Check the Docker logs for errors:
     ```bash
     docker-compose logs
     ```
2. **Port Conflicts**:
   - Update the `ports` section in the Compose file to resolve conflicts.


## Supporting Resources
- [Docker Compose Documentation](https://docs.docker.com/compose/)

<!-- - [Community Forum](https://community.example.com) -->
