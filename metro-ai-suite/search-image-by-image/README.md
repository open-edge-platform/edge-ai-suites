
This reference application showcases searching through video data generated by cameras around a city to find the closest match to the user-provided image.

<div align="center"><img src="./docs/developer-guide/images/sibi.gif"/></div>

### Introduction

The reference implementation utilizes Vision AI and third-party microservices to implement two processing pipelines. The Video Analysis pipeline converts live video steram from cameras into feature vectors (image embeddings) stored in Vector DB. The Video Search pipeline queries database to find objects in images captured from live video streams. It displays a list of frames that match the search input objects along with the metadata of each frame.

<div align="center"><img src="./docs/developer-guide/images/architecture.png" width="100%"/></div>

- **MediaMTX** : This (third-party) microservice simulates remote video cameras placed in various geographical locations. The service can replay videos from recorded files and publish them as live video streams in different formats. The RTSP stream is sent to EVAM microservice for extraction of feature vectors. The WebRTC stream is useful for displaying live video streams in this web application.

- **EVAM** : This microservice operates either in streaming mode (video analysis) or image ingestion mode (video search). During video analysis phase, it consumes RTSP streams from simulated remote cameras, and performs object detection and feature extraction tasks. The resulting predictions are published as metadata to an MQTT Broker. During video search phase, the microservice returns feature vectors for submitted images.

- **Feature Matching** : This microservice interacts with EVAM and Vector DB. During video analysis phase, it inserts feature vectors (image embeddings) generated by EVAM into Vector DB. Additionally, it stores frames which are useful for displaying in search results when a user searches for an object. During video search phase, the microservice queries EVAM to get feature vectors for user-provided image and then it queries Vector DB to find the matching frame(s) from video streams processed in analysis phase.

- **MQTT** : This (third-party) microservice transfers stream of metadata between EVAM and Feature Matching during video analysis phase.

- **MilvusDB** : This (third-party) microservice stores feature vectors in a Vector DB and provides a vector search to find the closest match between user-provided image and previously processed video frames.
 
### Installing behind a proxy

1. Follow this guide to configure your Docker environment: [Use a proxy server with the Docker CLI | Docker Documentation](https://docs.docker.com/engine/cli/proxy/)
2. Pull base images:

    ```
    docker pull docker.io/library/node:23
    docker pull docker.io/library/python:3.11
    docker pull docker.io/intel/dlstreamer:2024.2.2-ubuntu24
    docker pull openvino/ubuntu22_dev:2024.6.0
    ```
3. Make sure that proxy configuration is not propagated to all containers except streaming-pipeline. You can do this by setting `NO_PROXY`, `HTTP_PROXY` and `HTTPS_PROXY` environment variables to empty string in the `compose.yml` file for all services except streaming-pipeline. For example:

    ```yaml
    environment:
      - HTTP_PROXY=
      - HTTPS_PROXY=
      - NO_PROXY=
      - http_proxy=
      - https_proxy=
      - no_proxy=
    ```
    For streaming-pipeline service, you can set the proxy configuration as follows in the `compose.yml` file:

    ```yaml
    environment:
      - HTTP_PROXY=http://<proxy>:<port>
      - HTTPS_PROXY=http://<proxy>:<port>
      - NO_PROXY=localhost,127.0.0.1
      - http_proxy=http://<proxy>:<port>
      - https_proxy=http://<proxy>:<port>
    ```

### Build Instructions

Build containers:

```
docker compose build
```

Download Models:

<details open>
<summary>
Linux Instructions
</summary>


```sh
MODELS_PATH="$(pwd)/src/evam/models"

docker run --rm \
    -v $MODELS_PATH:/output \
    openvino/ubuntu22_dev:2024.6.0 bash -c \
    "omz_downloader --name resnet-50-pytorch --output_dir models && \
     omz_converter --name resnet-50-pytorch --download_dir models --output_dir models && \
     cp -r ./models/public/resnet-50-pytorch /output"

docker run --rm \
    -v $MODELS_PATH:/output \
    openvino/ubuntu22_dev:2024.6.0 bash -c \
    "omz_downloader --name person-vehicle-bike-detection-2004 --output_dir models && \
     omz_converter --name person-vehicle-bike-detection-2004 --download_dir models --output_dir models && \
     cp -r ./models/intel/person-vehicle-bike-detection-2004 /output"
```

</details>

<details>
<summary>
Windows Instructions
</summary>

```ps1
$MODELS_PATH="$PWD\src\evam\models"

docker run --rm `
    -v ${MODELS_PATH}:/output `
    openvino/ubuntu22_dev:2024.6.0 bash -c `
    "omz_downloader --name resnet-50-pytorch --output_dir models && `
     omz_converter --name resnet-50-pytorch --download_dir models --output_dir models && `
     cp -r ./models/public/resnet-50-pytorch /output"

docker run --rm `
    -v ${MODELS_PATH}:/output `
    openvino/ubuntu22_dev:2024.6.0 bash -c `
    "omz_downloader --name person-vehicle-bike-detection-2004 --output_dir models && `
     omz_converter --name person-vehicle-bike-detection-2004 --download_dir models --output_dir models && `
     cp -r ./models/intel/person-vehicle-bike-detection-2004 /output"
```

</details>

### Quick Evaluation

- Start Containers:

```sh
docker compose up -d 
```

The App has the following endpoints: 

* Stream UI: http://localhost:8889/stream
* App UI: http://localhost:3000
* Search UI: http://localhost:9000/docs
* Milvus UI: http://localhost:8000/

Here is an example output: 

<div align="center">
    <img src="./docs/developer-guide/images/imagesearch1.png" width="45%" style="margin-right:1rem"/>
    <img src="./docs/developer-guide/images/imagesearch2.png" width="45%" />
</div>

### References

- [Bill of Materials](BILL_OF_MATERIALS.md)  

### Kubernetes

Follow this instructions to deploy the app in Kubernetes (k8s) using Helm.

Deploy the application with the following commands:

```bash
# Install the Search Image by Image chart in the sibi namespace
helm upgrade \
    --install sibi ./chart \
    --create-namespace \
    -n sibi \
    --debug
```

Some containers in the deployment requires network access.
If you are in a proxy environment, pass the proxy environment variables as follows:

```bash
# Install the Search Image by Image chart in the sibi namespace
# Replace the proxy values with the specific ones for your environment:
helm upgrade \
    --install sibi ./chart \
    --create-namespace \
    --set httpProxy="http://proxy.example.com:8080" \
    --set httpsProxy="http://proxy.example.com:8080" \
    --set noProxy="localhost\,127.0.0.1" \
    -n sibi \
    --debug
```

To get the port where the application is serving, run the following command:

```bash
kubectl -n sibi get svc/sibi-app
```

This is an example output of the previous command:

```text
NAME       TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
sibi-app   NodePort   10.109.118.49   <none>        3000:31998/TCP   14m
```

Now frontend should be accessible at http://localhost:31998/.

Finally, the app can be uninstalled using the following command:

```bash
# And this is how you uninstall the chart:
helm uninstall -n sibi sibi
```

### Important Notice

This reference implementation is designed to facilitate smart cities application development. It is not intended for police, military or similar surveillance uses, use as part of critical infrastructure operations, determining access to education or other significant resource, managing workers or evaluating employment performance.

This reference implementation is intended to allow users to examine and evaluate Search Image by Image application and the associated performance of Intel technology solutions. The accuracy of computer models is a function of the relation between the data used to train them and the data that the models encounter after deployment. This model has been tested using datasets that may not be sufficient for use in production applications. Accordingly, while the model may serve as a strong foundation, Intel recommends and requests that this model be tested against data the model is likely to encounter in specific deployments.

Intel is committed to respecting human rights and avoiding causing or contributing to adverse impacts on human rights. See Intel’s Global Human Rights Principles. Intel’s products and software are intended only to be used in applications that do not cause or contribute to adverse impacts on human rights.

### Licensing Information

**FFmpeg**: FFmpeg is an open source project licensed under LGPL and GPL. See [FFmpeg Legal Information](https://www.ffmpeg.org/legal.html). You are solely responsible for determining if your use of FFmpeg requires any additional licenses. Intel is not responsible for obtaining any such licenses, nor liable for any licensing fees due, in connection with your use of FFmpeg.

**GStreamer**: GStreamer is an open source framework licensed under LGPL. See [GStreamer Licensing Information](https://gstreamer.freedesktop.org/documentation/frequently-asked-questions/licensing.html). You are solely responsible for determining if your use of GStreamer requires any additional licenses. Intel is not responsible for obtaining any such licenses, nor liable for any licensing fees due, in connection with your use of GStreamer.